% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrartcl}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother


% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}



\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{anyfontsize}
\KOMAoption{captions}{tableheading}
\include{_extensions/maths_shortcuts.tex}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Limited information goodness-of-fit tests for ordinal factor models},
  pdfkeywords={Ordinal data, Confirmatory factor analysis, Limited
information, Goodness-of-fit tests},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Limited information goodness-of-fit tests for ordinal factor
models}
\author{Haziq Jamil}
\date{}
\begin{document}
\maketitle
\begin{abstract}
Limited information approaches overcome sparsity issues and
computational challenges in traditional goodness-of-fit tests. This
paper describes the implementation of LIGOF tests for ordinal factor
models that have been fitted using the \texttt{\{lavaan\}} package in R.
The tests are computationally efficient and reliable, and adapted to
suit whichever parameter estimation procedure was used to fit the model.
The implementation is available as an R package called
\texttt{\{lavaan.ligof\}}.
\end{abstract}


\section{Introduction}\label{introduction}

\begin{itemize}
\tightlist
\item
  Focus on limited information methods that use up to second-order
  moments of the data.
\item
  This synergises well with the LIGOF tests which also use up to
  second-order moments.
\item
  \textbf{IF} full information tests are used, then there is still the
  computational burden of computing the full multinomial matrix
  \(\bSigma\) which grows exponentially with the number of variables.
  Using limited information methods to estimate the parameters offers a
  way to avoid this.
\item
  Besides, most software uses limited information methods to estimate
  the parameters of ordinal factor models, such as the
  \texttt{\{lavaan\}} package in R, Mplus, Stata, and LISREL.
\item
  Sometimes, GLS methods involve a fit function which are themselves
  asymptotically chi square, and this can be used for testing fit.
  However, more popular versions use thresholds and polychoric
  correlations, and in this case it is not possible to detect sources of
  misfit.
\item
  It would appear that calculation of LIGOF tests statistics involve
  quantities that are already computed in the process of estimating the
  parameters of the model, so it is not computationally burdensome to
  compute these tests.
\end{itemize}

\section{Methods}\label{methods}

\subsection{Ordinal data}\label{ordinal-data}

Consider the case of analysing multivariate data
\(\mathbf y = (y_{1}, \ldots, y_{p})^\top\), where each item \(y_{i}\)
is an ordinal random variable with \(m_i\) categories, \(i=1,\dots,p\).
Let
\(\mathcal R = \{ \mathbf c = (c_1,\dots, c_p)^\top \mid c_i \in \{1,\dots, m_i\}\}\)
be the set of all possible response patterns, and let
\(R=\prod_{i} m_i\) be the cardinality of this set. The joint
probability of observing a response pattern
\(\mathbf c_r \in \mathcal R\) is given by
\begin{equation}\phantomsection\label{eq-each-joint-resp-prob}{
\pi_r = \Pr(\mathbf y = \mathbf c_r) = \Pr(y_1 = \mathbf c_{r1}, \ldots, y_p = \mathbf c_{rp}), \hspace{2em} r = 1, \ldots, R,
}\end{equation} with \(\sum_r \pi_R = 1\). Collect all response
probabilities into the vector
\(\boldsymbol \pi = (\pi_1, \ldots, \pi_R)^\top \in [0,1]^R\). An
example with \(p=3\), \(m_1=2\), and \(m_2=m_3=3\) is given below. In
total, there are \(R=2 \times 3 \times 3 = 18\) response patterns as
shown in Table~\ref{tbl-response-patterns}.

\begin{table}

\caption{\label{tbl-response-patterns}Response patterns for \(p=3\) with
\(m_1=2\), and \(m_2=m_3=3\).}

\begin{minipage}{0.50\linewidth}

\fontsize{12.0pt}{14.4pt}\selectfont
\begin{tabular*}{0.8\linewidth}{@{\extracolsep{\fill}}rrrrr}
\toprule
\(r\) & \(y_1\) & \(y_2\) & \(y_3\) & Pattern \\ 
\midrule\addlinespace[2.5pt]
1 & 1 & 1 & 1 & 111 \\ 
2 & 1 & 1 & 2 & 112 \\ 
3 & 1 & 1 & 3 & 113 \\ 
4 & 1 & 2 & 1 & 121 \\ 
5 & 1 & 2 & 2 & 122 \\ 
6 & 1 & 2 & 3 & 123 \\ 
7 & 1 & 3 & 1 & 131 \\ 
8 & 1 & 3 & 2 & 132 \\ 
9 & 1 & 3 & 3 & 133 \\ 
\bottomrule
\end{tabular*}

\textsubscript{Source:
\href{https://haziqj.github.io/ligof-tests/manuscript.qmd.html}{Article
Notebook}}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\fontsize{12.0pt}{14.4pt}\selectfont
\begin{tabular*}{0.8\linewidth}{@{\extracolsep{\fill}}rrrrr}
\toprule
\(r\) & \(y_1\) & \(y_2\) & \(y_3\) & Pattern \\ 
\midrule\addlinespace[2.5pt]
10 & 2 & 1 & 1 & 211 \\ 
11 & 2 & 1 & 2 & 212 \\ 
12 & 2 & 1 & 3 & 213 \\ 
13 & 2 & 2 & 1 & 221 \\ 
14 & 2 & 2 & 2 & 222 \\ 
15 & 2 & 2 & 3 & 223 \\ 
16 & 2 & 3 & 1 & 231 \\ 
17 & 2 & 3 & 2 & 232 \\ 
18 & 2 & 3 & 3 & 233 \\ 
\bottomrule
\end{tabular*}

\textsubscript{Source:
\href{https://haziqj.github.io/ligof-tests/manuscript.qmd.html}{Article
Notebook}}

\end{minipage}%

\end{table}%

Later on we wish to use lower-order residuals to assess the fit of a
model to the data, which first requires a description of lower-order
moments and its connection to the joint response probabilities.
Marginally, each \(y_i\) can be viewed as a multinoulli random variable
with event probabilities \(\pi^{(i)}_k = \Pr(y_i = k)\),
\(k=1,\dots m_i\), that sum to one. Therefore, this univariate
distribution is characterised by its \((m_i-1)\) \emph{moments}
\(\pi^{(i)}_2,\dots,\pi^{(i)}_{m_i}\), with the first moment being
redundant due to the sum to unity constraint. All univariate moments can
be collected into the vector
\(\dot{\boldsymbol\pi}_1 = (\pi^{(i)}_k)^\top\) whose dimension is
\(S_1 = \sum_i (m_i-1)\). In a similar light, the bivariate distribution
of \((y_i, y_j)\) is characterised by its \((m_i-1)(m_j-1)\) \emph{joint
moments} \(\pi^{(ij)}_{k,l} = \Pr(y_i = k, y_j = l)\),
\(k=2,\dots,m_i\), \(l=2,\dots,m_j\). Also collect all bivariate moments
into the vector \(\dot{\boldsymbol\pi}_2 = (\pi^{(ij)}_{k,l})^\top\)
whose dimension is \(S_2 = \sum_{i<j} (m_i-1)(m_j-1)\). Finally, denote
by \(\boldsymbol\pi_2 = (\dot\bpi_1^\top, \dot\bpi_2^\top)^\top\) the
vector of multivariate moments up to order 2, which is a vector of
length \(S = S_1 + S_2\).

Because the lower order moments are contained in the higher order
moments, the vector \(\boldsymbol\pi_2\) can be extracted from the joint
probabilities \(\bpi\) via a linear operation \(\bpi_2 = \bT_2 \bpi\)
(Jamil et al., 2025). As an example, continuing from the \(p=3\)
instance above, the moments for the first variable \(y_1\),
\(\Pr(y_1=2)\) can be obtained by \emph{summing} over all joint
probabilities whose patterns contain \(y_1=2\). The positions of these
joint probabilities in the vector \(\bpi\) are picked up by the first
row of the matrix \(\bT_2\). Similarly, the two bivariate moments of
\((y_1,y_2)\), i.e.~\(\pi^{(12)}_{22}\) and \(\pi^{(12)}_{23}\) are
obtained by summing over the joint probabilities whose patterns contain
\(y_1=2\) and \(y_2=2\), and \(y_1=2\) and \(y_2=3\), respectively.

\begin{figure}

\centering{

\begin{verbatim}
          111 112 113 121 122 123 131 132 133 211 212 213 221 222 223 231 232 233
Y1=2        0   0   0   0   0   0   0   0   0   1   1   1   1   1   1   1   1   1
Y2=2        0   0   0   1   1   1   0   0   0   0   0   0   1   1   1   0   0   0
Y2=3        0   0   0   0   0   0   1   1   1   0   0   0   0   0   0   1   1   1
Y3=2        0   1   0   0   1   0   0   1   0   0   1   0   0   1   0   0   1   0
Y3=3        0   0   1   0   0   1   0   0   1   0   0   1   0   0   1   0   0   1
Y1=2,Y2=2   0   0   0   0   0   0   0   0   0   0   0   0   1   1   1   0   0   0
Y1=2,Y2=3   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1   1
Y1=2,Y3=2   0   0   0   0   0   0   0   0   0   0   1   0   0   1   0   0   1   0
Y1=2,Y3=3   0   0   0   0   0   0   0   0   0   0   0   1   0   0   1   0   0   1
Y2=2,Y3=2   0   0   0   0   1   0   0   0   0   0   0   0   0   1   0   0   0   0
Y2=2,Y3=3   0   0   0   0   0   1   0   0   0   0   0   0   0   0   1   0   0   0
Y2=3,Y3=2   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   1   0
Y2=3,Y3=3   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   1
\end{verbatim}

\textsubscript{Source:
\href{https://haziqj.github.io/ligof-tests/manuscript.qmd.html}{Article
Notebook}}

}

\caption{\label{fig-T2-matrix}Matrix \(\bT_2\) for the case of \(p=3\)
with \(m_1=2\), and \(m_2=m_3=3\).}

\end{figure}%

Note that this construction of lower-order moments generalises to any
order \(q \le p\), but the total number of moments up to order \(q\)
grows combinatorially in both \(p\) and the category counts \(m_i\),
yielding design matrices \(\mathbf{T}_q\) that can become
computationally burdensome. Moreover, although we arbitrarily dropped
the first moment in the foregoing construction, the choice of which
category to omit is immaterial. This is because category probabilities
sum to one, so excluding any one category produces a similar-dimensional
parameterisation algebraically equivalent to excluding any other. For
further details, consult Reiser (1996) and Maydeu-Olivares \& Joe
(2006).

\subsection{Confirmatory factor
analysis}\label{confirmatory-factor-analysis}

The confirmatory factor analysis (CFA) model imposes a structure on the
joint response probabilities by assuming that the \(p\) observed
variables are manifestations of a smaller set of \(q\) latent variables.
In this way, the CFA may be viewed as a data-reduction technique since,
effectively, the correlations among variables are modelled by a
pre-specific factor structure using lower-dimensional data summaries.

CFA is typically used for continuous manifest variables, but it can also
be applied to ordinal data. A common approach is the \emph{underlying
variable} (UV) approach, where the observed responses \(y_i\) are
assumed to be discretised versions of continuous latent variables
\(y_i^*\). The connection is made through \[
y_i = \begin{cases}
1 & \ \ \tau_0^{(i)} < y^*_i < \tau_1^{(i)} \\
2 &  \ \ \tau_{1}^{(i)} <  y^*_i < \tau_2^{(i)} \\
3 &  \ \ \tau_{2}^{(i)} <  y^*_i < \tau_3^{(i)} \\
\vdots &  \hphantom{\tau_{1}^{(i)} \leq \ \ \ } \vdots \\
m_i & \tau_{m_i-1}^{(i)} < y^*_i < \tau_{m_i}^{(i)},
\end{cases}
\] with the \emph{thresholds} \(\tau_k^{(i)}\) for item \(i\) satisfying
the ordering \[
-\infty \equiv \tau_0^{(i)} < \tau_1^{(i)} < \tau_2^{(i)} < \cdots < \tau_{m_i-1}^{(i)} < \tau_m^{(i)} \equiv +\infty.
\] Evidently, the model is invariant to a linear transformation, since
scaling and shifting the underlying variables \(y_i^*\) do not affect
the outcome of the ordinal variable \(y_i\). For this reason it is
convenient to assume, for the purposes of model identifiability, a zero
mean Gaussian distribution \(\by^* \sim \N_p(\bzero,\bSigma_{\by^*})\),
where \(\bSigma_{\by^*}\) is a correlation matrix.

The underlying continuous variables \(\by^*\), unlike their discrete
counterparts \(\by\), are now suitable to be modelled using a factor
analysis model. Here, the goal is to find a set of latent factors
\(\bfeta = (\eta_1,\dots,\eta_q)^\top \in \bbR^q\), with \(q \ll p\),
that sufficiently explain the covariance structure of the
\(p\)-dimensional variable space. This is achieved by the relationship
\[
\by^* = \bLambda \bfeta + \bepsilon,
\] where \(\bLambda\) is a (often sparse) \(p \times q\) matrix of
factor loadings, and \(\bepsilon\) is a vector of residuals. Certain
distributional assumptions are made, namely that
\(\bfeta \sim \N_q(\bzero,\bPsi)\) with \(\bPsi\) a correlation matrix,
\(\bepsilon \sim \N_p(\bzero,\bTheta_{\bepsilon})\) with
\(\bTheta_{\bepsilon} = \bI - \diag(\bLambda \bPsi \bLambda^\top)\), and
that \(\Cov(\bfeta,\bepsilon) = \bzero\). Together, this implies that
the polychoric correlation matrix of \(\by\) is given by \[
\bSigma_{\by^*} = \bLambda \bPsi \bLambda^\top + \bTheta_{\bepsilon} \in \bbR^{p\times p}.
\] As a remark, the UV approach is commonly employed in the context of
confirmatory factor analysis (CFA) models due to the ease of modelling,
though other approaches such as item response theory (IRT) models are
also available (Jöreskog \& Moustaki, 2001).

For this factor analysis model, the parameters of interest are the
non-zero entries \(\blambda\) of the loading matrix \(\bLambda\), the
unique non-diagonal entries \(\bpsi\) in the factor correlation matrix
\(\bPsi\), and the thresholds
\(\btau^{(i)} = (\tau_1^{(i)},\dots,\tau_{m_i-1}^{(i)})^\top\) for each
ordinal item \(y_i\). Collectively, these parameters are denoted by
\(\theta = (\blambda^\top,\brho^\top,\btau^{(1)},\dots,\btau^{(p)})^\top\)
belonging to some parameter space \(\Theta\). Under this CFA model, each
joint response probability \(\pi_r\) from
Equation~\ref{eq-each-joint-resp-prob} is now evaluated as a function of
\(\theta\):
\begin{equation}\phantomsection\label{eq-joint-resp-prob-integral}{
\pi_r := \pi_r(\theta) = \idotsint \limits_{\mathcal C_r} \phi_p(\by^* \mid \bzero,\bSigma_{\by^*}) \, \dint\by^*,
}\end{equation} where the \(p\)-dimensional integral is taken over the
set
\(\mathcal C_r = \{ \by^* \in \bbR^p \mid y_i = \mathbf c_{ri}, i=1,\dots,p\}\),
the set of all continuous values that yield the response pattern
\(\mathbf c_r\).

\subsection{\texorpdfstring{Consistent estimators for
\(\theta\)}{Consistent estimators for \textbackslash theta}}\label{consistent-estimators-for-theta}

Suppose that a sample \(\mathcal Y = \{\by^{(h)}\}_{h=1}^n\) is
obtained, where \(\by^{(h)} = (y_1^{(h)},\ldots,y_p^{(h)})^\top\)
represents the \(p\)-dimensional ordinal-data observation from subject
\(h\in\{1,\dots,n\}\). As a remark, samples may not necessarily be
independent, and in such cases, corresponding sampling weights
\(\omega_s\) can be used to account for the sampling design (Jamil et
al., 2025), and most of what will be discussed below can be adapted to
account for this.

Many methods exist to estimate the parameters \(\theta\) of the CFA
model, but we are most interested in those that yield a
\(\sqrt{n}\)-consistent and asymptotically normal estimator.
Specifically, we assume that \(\hat\theta\) satisfies
\begin{equation}\phantomsection\label{eq-theta-consistent}{
\begin{aligned}
\sqrt{n}(\hat\theta - \theta) = \hat\bQ \cdot \sqrt{n}(\bp - \bpi(\theta)) + o_p(1),
\end{aligned}
}\end{equation} where the term \(\bp = (p_1,\ldots,p_R)^\top\) is the
vector of empirical joint response proportions, and
\(\hat\bQ \xrightarrow{\text P} \bQ\) as \(n\to\infty\) is some
\emph{influence matrix} that performs asymptotic linearisation from the
joint response proportions \(\bp\) to the parameters \(\theta\). This
includes a wide range of likelihood-based (Bock \& Lieberman, 1970;
Lord, 1968) and pseudolikelihood-based (Alfonzetti et al., 2025;
Katsikatsou et al., 2012) methods, as well as generalised least squares
(GLS) based methods (Christoffersson, 1975; Jöreskog, 1990, 1994;
Jöreskog \& Moustaki, 2001; Muthén, 1978, 1984), with GLS popularly
implemented as a multi-stage estimation procedure in software.
Equation~\ref{eq-theta-consistent} holds true whether full information
methods (i.e., estimation using joint response probabilities) or limited
information methods (i.e., using a lower-order subset of the response
probabilities) are employed.

A neat way of viewing the parameter estimation is that most of these
methods are a class of M-estimators. M-estimation provides a general and
flexible framework for parameter estimation, in which estimators are
obtained by minimizing an objective function \(F(\theta)\), typically
expressed as an empirical average \(\sum_{s=1}^n F(\by_s, \theta)\), or,
equivalently, by solving a system of estimating equations
\(\sum_{s=1}^n \nabla_\theta F(\by_s, \theta) = \bzero\), where
\(\nabla_\theta F = \partial F / \partial \theta\). This formulation
encompasses a wide range of classical and robust procedures, including
maximum likelihood, least squares, and weighted least squares methods
mentioned above.

In the context of confirmatory factor analysis (CFA) with ordinal
indicators, the estimating equations typically arise from a discrepancy
function defined on thresholds and polychoric correlations, and
M-estimation offers a principled way to derive estimators even when the
full likelihood is computationally intractable. A central assumption in
this framework is that there exists a parameter \(\theta_0 \in \Theta\)
such that the population moment condition
\(\E[\nabla_\theta F(\by, \theta_0)] = 0\) holds. This condition is not
a consequence of the data, but rather a theoretical premise about the
underlying data-generating mechanism. It defines the parameter value to
which the estimator is expected to converge. In a correctly specified
model, \(\theta_0\) corresponds to the true parameter; in the presence
of misspecification, it instead represents the value that best satisfies
the moment condition within the assumed model class.

Under standard regularity conditions---such as continuity of
\(\nabla_\theta F\) in \(\theta\), measurability, and uniform
convergence of empirical averages---the M-estimator \(\hat\theta\) is
consistent and asymptotically normal (Huber, 1964; van der Vaart, 1998).
Specifically, \[
\sqrt{n}(\hat\theta - \theta) \xrightarrow{\text D} \N(\bzero, \bV(\theta)),
\] where the asymptotic variance is given by the sandwich formula
\(\bV(\theta) = \cH(\theta)^{-1} \cJ(\theta) \cH(\theta)^{-T}\), with \[
\cH(\theta) = \E \left[ - \nabla_\theta^2 \, F(\by,\theta) \right], \quad
\cJ(\theta) = \E \left[ \nabla_\theta \,F(\by,\theta) \ \nabla_\theta \, F(Y,\theta) ^\top \right].
\] The matrix \(\cH\) is known as the \emph{sensitivity matrix} and is
estimated consistently by
\(\hat\bH = -\frac{1}{n} \sum_{s=1}^n \nabla_\theta^2 \, F(\by_s, \hat\theta)\).
The matrix \(\cJ\) is known as the \emph{variability matrix} and is
estimated consistently by
\(\hat\bJ = \frac{1}{n} \sum_{s=1}^n \nabla_\theta \, F(\by_s, \hat\theta) \nabla_\theta \, F(\by_s, \hat\theta)^\top\).

These properties make M-estimation particularly appealing in settings
where the data are ordinal and the working model may be misspecified, as
is often the case in large-scale psychometric applications. For a
detailed treatment of the asymptotic theory of M-estimators in
econometric and semiparametric contexts, see Newey \& McFadden (1994).
For the commonly used techniques to estimate CFA, the table below gives
an overview for the form that \(F\) and its derivatives take.

\begin{table}

\caption{\label{tbl-objfun-deriv}Objective functions and their
derivatives for different estimators.}

\centering{

\fontsize{12.0pt}{14.4pt}\selectfont
\begin{tabular*}{0.9\linewidth}{@{\extracolsep{\fill}}l|lll}
\toprule
 & \(\mathbf m - \boldsymbol\mu(\theta)\) & \(\mathbf W^{-1}\) & \(\mathbf D(\theta)\) \\ 
\midrule\addlinespace[2.5pt]
ML\textsuperscript{\textit{1}} & \(\mathbf p - \boldsymbol\pi(\theta)\) & \(\operatorname{diag}(\boldsymbol\pi(\theta))\) & \(\partial \boldsymbol\pi(\theta) / \partial \theta\) \\ 
PML\textsuperscript{\textit{2}} & \(\mathbf p_{\text{pair}} - \boldsymbol\pi_{\text{pair}}(\theta)\) & \(\operatorname{diag}(\boldsymbol\pi_{\text{pair}}(\theta))\) & \(\partial \boldsymbol\pi_{\text{pair}}(\theta) / \partial \theta\) \\ 
UBN\textsuperscript{\textit{3}} & \(\mathbf p_2 - \boldsymbol\pi_2(\theta)\) & \(\operatorname{diag}(\boldsymbol\pi_2(\theta))\) & \(\partial \boldsymbol\pi_2(\theta) / \partial \theta\) \\ 
MCS\textsuperscript{\textit{4}} & \(\mathbf p - \boldsymbol\pi(\theta)\) & \(\boldsymbol\Sigma\) & \(\partial \boldsymbol\pi(\theta) / \partial \theta\) \\ 
MCS2\textsuperscript{\textit{4}} & \(\mathbf p_2 - \boldsymbol\pi_2(\theta)\) & \(\boldsymbol\Sigma_2\) & \(\partial \boldsymbol\pi_2(\theta) / \partial \theta\) \\ 
ULS\textsuperscript{\textit{5}} & \(\mathbf s - \boldsymbol\sigma(\theta)\) & \(\mathbf I\) & \(\partial \boldsymbol\sigma(\theta) / \partial \theta\) \\ 
WLS\textsuperscript{\textit{5}} & \(\mathbf s - \boldsymbol\sigma(\theta)\) & \(\boldsymbol\Gamma=\operatorname{limvar}(\mathbf s - \boldsymbol\sigma(\theta))\) & \(\partial \boldsymbol\sigma(\theta) / \partial \theta\) \\ 
DWLS\textsuperscript{\textit{5}} & \(\mathbf s - \boldsymbol\sigma(\theta)\) & \(\operatorname{diag}(\boldsymbol\Gamma)\) & \(\partial \boldsymbol\sigma(\theta) / \partial \theta\) \\ 
\bottomrule
\end{tabular*}
\begin{minipage}{\linewidth}
\textsuperscript{\textit{1}}Maximum likelihood (Lord 1968; Bock and Lieberman 1970)\\
\textsuperscript{\textit{2}}Pairwise maximum likelihood (Katsikatsou, Moustaki, Yang-Wallentin, and Jöreskog 2012). \(\mathbf p_{\text{pair}}\) and \(\boldsymbol\pi_{\text{pair}}(\theta)\) vectors of sample and model-implied pairwise response probabilities, respectively.\\
\textsuperscript{\textit{3}}Underlying bivariate normal approach (Jöreskog and Moustaki 2001)\\
\textsuperscript{\textit{4}}Minimum chi square (Christoffersson 1975; Muthén 1978)\\
\textsuperscript{\textit{5}}Unweighted, weighted, or diagonally weighted least squares (Muthén 1984; Jöreskog 1990, 1994). \(\mathbf s\) and \(\boldsymbol\sigma(\theta)\) vectors of sample and model-implied thresholds and polychoric correlations, respectively.\\
\end{minipage}

}

\end{table}%

\textsubscript{Source:
\href{https://haziqj.github.io/ligof-tests/manuscript.qmd.html}{Article
Notebook}}

Achieving the desired form stated in Equation~\ref{eq-theta-consistent}
requires an asymptotic linearisation argument. For CFA models, a general
M-estimator \(\hat\theta\) for \(\theta\) is obtained by solving the set
of estimating equations \[
U(\theta) := n \bD(\theta)^\top \bW_{\theta} (\bmm - \mu(\theta)) = 0
\] where \(\bmm\) is a vector of sample moments, \(\mu(\theta)\) is the
vector of model-implied moments, \(\bD(\theta)\) is the Jacobian of the
model-implied moments with respect to \(\theta\), and \(\bW_{\theta}\)
is a weight matrix which may or may not depend on the parameters.
Table~\ref{tbl-objfun-deriv} summarises these quantities for different
estimators most commonly used for CFA. Under correct model
specification, the sensitivity matrix takes the form \[
\cH(\theta) = \bD(\theta)^{\top} \bW_{\theta}  \bD(\theta).
\]

A first-order Taylor expansion of \(U(\hat\theta)\) around \(\theta\)
with a little rearranging and multiplying through by \(\sqrt n\) gives
\[
\sqrt{n}\,(\hat\theta - \theta)
=
\left[ - \frac{1}{n} \frac{\partial U(\theta)}{\partial\theta} \right]^{-1}
\bD(\theta)^\top \bW_{\theta} \cdot \sqrt n (\bmm - \mu(\theta))
+ o_p(1),
\] where the observed Hessian
\(-\frac{1}{n}\partial U(\theta) / \partial\theta \xrightarrow{\text P} \cH(\theta)\)
as \(n \to \infty\). Taking limits, we see the influence matrix for CFA
shaping up to involve \[
\left[ - \frac{1}{n} \frac{\partial U(\theta)}{\partial\theta} \right]^{-1}
\bD(\theta)^\top \bW_{\theta} \xrightarrow{\text P} \cH(\theta)^{-1} \bD(\theta)^\top \bW_{\theta} =: \tilde \bQ \quad \text{ as } n\to\infty.
\]

For certain full-information estimators like ML and MCS, \(\tilde \bQ\)
fits in to be premultiplied to the the \(R\)-vector of moment
differences \(\bmm - \bmu(\theta) = \bp - \bpi(\theta)\), and thus
Equation~\ref{eq-theta-consistent} is satisfied. Incidentally, in the
case of ML, the influence matrix is
\(\bQ = \cI^{-1} \bDelta \bW \in \bbR^{t \times R}\), where
\(\cI= \bDelta^\top \bW^{-1} \bDelta^\top\) is the unit Fisher
information, \(\bDelta= (\partial \bpi / \partial \theta)\) is the
Jacobian of the joint response probabilities with respect to the
parameters, and \(\bW = \diag(\bpi)\) is a diagonal matrix of the joint
response probabilities, agreeing with results in Maydeu-Olivares \& Joe
(2005). It can be further shown that \(\bQ\) simplifies to
\(\bDelta^\top \cI^{-1} \bDelta\).

In other cases, we need to post multiply the influence matrix
\(\tilde\bQ\) by an appropriate matrix so that it is able to conform to
a matrix-vector multiplication with the joint probabilities as per
Equation~\ref{eq-theta-consistent}. This depends on the vector of moment
differences. Consider a transformation \(g: \bp \mapsto \bmm\) that maps
the joint response probabilities \(\bp\) to the moments \(\bmm\) (and
likewise for the model implied moments), and let
\(\bG := \partial g / \partial \bp\) be the Jacobian of the
transformation. When dealing with PML, UBN or MCS2, then the
transformation is linear since the lower order moments are linear
functions of the joint response probabilities. On the other hand, ULS,
WLS, and DWLS methods specify a transformation that is not linear, where
the joint probabilities are mapped to the thresholds and polychoric
correlations. Such a transformation was described by Muthén (1978) in
the context of dichotomous data, but extends to the case of ordinal data
too. In any case,
\begin{equation}\phantomsection\label{eq-moments-transform}{
\begin{aligned}
\sqrt n (\bmm - \bmu(\theta)) 
&= \sqrt n \big(g(\bp) - g(\bpi(\theta))\big) \\
&= \bG \sqrt n \big(\bp - \bpi(\theta)\big) + o_p(1).
\end{aligned}
}\end{equation} Plugging this into the above equation lets us see the
form of the influence matrix as
\(\bQ = \cH(\theta)^{-1} \bD(\theta)^\top \bW_{\theta} \bG\).

When using limited information methods, it would be sufficient to
consider the lower-order moments transformation
\(g_2: \bp_2 \mapsto \bmm\) instead. For PML, UBN, and MCS2 this is
clearly obvious. For ULS, WLS, and DWLS, this also makes sense because
the the thresholds and polychoric correlations are functions of
univariate and bivariate moments respectively. Letting
\(\bG_2 := \partial g_2 / \partial \bp_2\), we have
\begin{equation}\phantomsection\label{eq-moments-transform2}{
\begin{aligned}
\sqrt n (\bmm - \bmu(\theta)) 
&= \sqrt n \big(g(\bp_2) - g(\bpi_2(\theta))\big) \\
&= \bG_2 \sqrt n \big(\bp_2 - \bpi_2(\theta)\big) + o_p(1) \\
&= \bG_2\bT_2 \sqrt n \big(\bp - \bpi(\theta)\big) + o_p(1),
\end{aligned}
}\end{equation} and thus Equation~\ref{eq-theta-consistent} holds with
the influence matrix \(\bQ = \bQ_2\bT_2\), where
\(\bQ_2 = \mathcal H(\theta)^{-1} \bD(\theta)\bW_\theta \bG_2\).
Consequently, when using limited information methods to estimate the
parameters of a CFA model, it is sufficient to consider only consistency
in relation to univariate and bivariate probabilities. We will see that
this is useful when we come to the topic of residuals.

\subsection{Distribution of residuals}\label{distribution-of-residuals}

Let \(p_r = n_r / n\) be the \(r\)th entry of the \(R\)-vector of sample
proportions \(\bp\), where \(n_r\) is the number of times the response
pattern \(\mathbf c_r\) was observed in the sample \(\mathcal Y\). The
random vector \(\bn = (n_1,\dots,n_R)^\top\) follows a multinomial
distribution with parameters \(n\), \(R\), and \(\bpi\), with
\(\E(\bn)=n\bpi\) and variance \[
\Var(\bn) = n (\diag(\bpi) - \bpi \bpi^\top) = n \bSigma.
\] It is widely known (Agresti, 2002) for iid samples that
\begin{equation}\phantomsection\label{eq-clt-prop}{
\sqrt{n} (\bp - \bpi) \xrightarrow{D} {\N}_R(\bzero, \bSigma)
}\end{equation} as \(n\to\infty\), which is a consequence of the central
limit theorem. Note that this also works for the case of weighted
samples in complex sampling designs, but \(\bSigma\) need not take a
multinomial form in such cases (Fuller, 2009).

Consider testing the composite null hypothesis of
\(\text{H}_0: \bpi = \bpi(\theta_0)\) against the alternative
\(\text{H}_1:  \bpi \neq \bpi(\theta_0)\). To do so, use the univariate
and bivariate residuals
\(\hat\be_2 = \bT_2(\bp - \bpi(\hat\theta)) = \bT_2 \hat \be\) as the
basis for the test statistic. Now we derive the asymptotic distribution
of this quantity. Write \[
\begin{aligned}
\sqrt n \, \hat\be
&= \sqrt n \, (\bp - \bpi(\theta_0)) - \sqrt n \, (\bpi(\hat\theta) - \bpi(\theta_0)) \\
&= \sqrt n \, (\bp - \bpi(\theta_0)) - \sqrt n \, \bDelta (\hat\theta - \theta_0) + o_p(1),
\end{aligned}
\] where we had considered a Taylor expansion of \(\bpi(\hat\theta)\)
around \(\theta_0\) to get to the second line, and defined
\(\bDelta = \big(\partial \bpi(\theta) / \partial \theta \big)\). Now,
for \(\sqrt n\)-consistent estimators satisfying
Equation~\ref{eq-theta-consistent}, we have that \[
\begin{aligned}
\sqrt n \, \hat\be
&= \sqrt n \, (\bp - \bpi(\theta_0)) -  \bDelta \hat\bQ  \cdot \sqrt{n} \, (\bp - \bpi(\theta_0)) + o_p(1) \\
&= (\bI - \bDelta \hat\bQ) \cdot \sqrt n \, (\bp - \bpi(\theta_0)) + o_p(1),
\end{aligned}
\] so it is clear that \(\hat\be\) is asymptotically normal by the CLT
(Equation~\ref{eq-clt-prop}). Let
\(\operatorname{limvar}(\hat\be) = \bOmega\). Then, since
\(\hat\be_2 = \bT_2 \hat\be\), the lower-order residuals are also
asymptotically normal with zero mean and variance
\(\bOmega_2 = \bT_2 \bOmega \bT_2^\top\). The full form of the
asymptotic variance is given by
\begin{equation}\phantomsection\label{eq-omega2}{
\begin{aligned}
\bOmega_2 = 
  \bSigma_2 
  - \bDelta_{2} \bQ \bSigma \bT_2^\top
  - \bT_2 \bSigma \bQ^\top \bDelta_{2}^\top 
  + \bDelta_{2} \bQ \bSigma \bQ^\top \bDelta_{2}^\top,
\end{aligned}
}\end{equation} where \(\bDelta_2 = \bT_2 \bDelta\). See Maydeu-Olivares
\& Joe (2008) for further details, including the use of residuals from
moments of up to order \(q < p\).

In practice, when limited informationntroduces inconsistency between the
estimation method and the quantities derived from it, potentially
leading to misleading inferences or misinterpretation of model fit.
methods are used to estimate the parameters, the estimation of
\(\bOmega_2\) involves plugging in full information quantities such as
fitted probabilities and Jacobians. This is less than ideal, since it
introduces inconsistency between the estimation method and the
quantities derived from it, potentially leading to misleading inferences
or misinterpretation of model fit. Furthermore, quantities such as the
multinomial covariance matrix \(\bSigma\) becomes exponentially large in
dimension as \(p\) increases, making it difficult to work with.

One solution is to consider the weaker \(\sqrt n\)-consistent condition
for limited information estimators suggested by
Equation~\ref{eq-moments-transform2}, in which the influence matrix
\(\bQ_2\) is utilised. Since \(\bQ = \bQ_2 \bT_2\),
Equation~\ref{eq-omega2} will simplify to
\begin{equation}\phantomsection\label{eq-omega2alt}{
\begin{aligned}
\bOmega_2 
  &= \bSigma_2 
  - \bDelta_{2} \bQ_2 \bSigma_2
  - \bSigma_2  \bQ_2^\top \bDelta_{2}^\top
  + \bDelta_{2} \bQ_2 \bSigma \bQ_2^\top \bDelta_{2}^\top \\
  &= (\bI - \bDelta_{2} \bQ_2) \bSigma_2 (\bI - \bDelta_{2}\bQ_2)^\top.
\end{aligned}
}\end{equation} where \(\bSigma_2 = \bT_2 \bSigma \bT_2^\top\) is the
covariance matrix of the lower-order moments. Computationally this is
more efficient as it uses only quantities involving uni and bivariate
moments, which are much smaller in size than the full joint response
probabilities.

\subsection{Wald-type tests}\label{wald-type-tests}

Given as \(\hat\be_2 \xrightarrow{\text D} \N_S(\bzero, \bOmega_2)\), we
can construct a Wald test statistic for the null hypothesis
\(\text{H}_0: \bpi = \bpi(\theta_0)\) as \[
X^2 = n \, \hat\be_2^\top \hat\bOmega_2^{-1} \hat\be_2,
\] where \(\hat\bOmega_2\) is a consistent estimator of \(\bOmega_2\).
This test statistic is asymptotically distributed as chi square under
the null hypothesis, with degrees of freedom equal to \(S-t\), i.e.~the
number of lower-order moments used in the test minus the number of
parameters estimated.

The computational challenges here are in the estimation of
\(\hat\bOmega_2\) as well as the inversion of the matrix. Addressing the
second issue first, suppose an estimator \(\hat\bOmega_2\) is available,
then the Moore-Penrose pseudoinverse \(\hat\bOmega_2^+\) can be computed
using the singular value decomposition (SVD) of \(\hat\bOmega_2\). This
sidesteps any numerical instabilities that may occur when inverting the
matrix directly, since the rank of \(\bOmega_2\) may be deficient
(Reiser, 1996), although inversion can still be computationally
challenging when the dimension \(S\) is large.

Jamil et al. (2025) instead proposed a diagonal Wald test, in which
\(\diag(\hat\bOmega_2)^{-1}\) is used instead of the full matrix
inverse. Since inverting a diagonal matrix is straightforward compared
to the full (pseudo) inverse, this is indeed computationally efficient.
However, simulation stadies show that this is not as powerful as the
full Wald test, in the context of pairwise likelihood estimation of
binary CFA models.

On the estimation of \(\bOmega_2\), which involves estimation of the
\(\bQ\) matrix, which may be involved depending on the estimation method
used. A very attractive proposal by Maydeu-Olivares and colleagues
(Maydeu-Olivares \& Joe, 2005, 2006, 2008) is to consider using a matrix
\(\bXi\) such that \(\bOmega_2\) is a generalised inverse of \(\bXi\),
i.e.~\(\bXi = \bXi \bOmega_2 \bXi\). By denoting
\(\bDelta_{2,\pi}^\perp\) to be an \(S\times (S-t)\) orthogonal
complement to \(\bDelta_{2,\pi}\) satisfying
\(\bDelta_{2,\pi}^\perp \bDelta_{2,\pi}^\top = \bzero\), it can be shown
that \(X^2 = \hat\be_2^\top \hat\bXi \hat\be_2\) converges to the Wald
test statistic with similar degrees of freedom (Jamil et al., 2025),
where \[
\bXi = \bDelta_{2,\pi}^\perp \big( (\bDelta_{2,\pi}^\perp)^\top \bSigma_2 \bDelta_{2,\pi}^\perp \big)^{-1} (\bDelta_{2,\pi})^\top.
\] This is advantageous in that it does not require the estimation of
\(\bQ\), and only requires the Jacobian \(\bDelta_{2,\pi}\) as well as a
consistent estimator for \(\bSigma_2\), which can be obtained from a
plug-in estimator using the model-implied probabilities
\(\bpi(\hat\theta)\).

\subsection{Pearson and general LIGOF
tests}\label{pearson-and-general-ligof-tests}

Wald-type tests may behave unstably and has poor small-sample behaviour
(Jamil et al., 2025). As an alternative, a Pearson-type test can be
constructed using the Pearson residuals \[
\begin{aligned}
X^2 
&= n \, \hat\be_2^\top \diag(\bpi_2(\hat\theta))^{-1} \hat\be_2 \\
&= 
n \sum_{i,k} \frac{p_k^{(i)} - \pi_k^{(i)}(\hat\theta)}{\pi_k^{(i)}(\hat\theta)} +
n \sum_{i<j}\sum_{k<l} \frac{p_{k,l}^{(ij)} - \pi_{k,l}^{(ij)}(\hat\theta)}{\pi_{k,l}^{(ij)}(\hat\theta)},
\end{aligned}
\] where \(p_k^{(i)}\) and \(p_{k,l}^{(ij)}\) are the sample estimates
for the univariate and bivariate response probabilities defined earlier.
Similar test statistics were studied by Cai et al. (2006) and
Bartholomew \& Leung (2002), where the latter considered only bivariate
margins. The Pearson test statistic does not follow an asymptotic
chi-square distribution because of the dependence of the summands in the
above equation. It does however converge to a sum of scaled chi-square
variables \(\sum_{s=1}^S \delta_s Z_s\), where each
\(Z_s \iid \chi^2_1\) and \(\delta_s\) are the eigenvalues of
\(M=\bOmega_2^{-1/2} \diag(\bpi_2(\hat\theta))^{-1} \bOmega_2^{-1/2}\).

For calculation of p-values, a moment matching procedure can be employed
(Jamil et al., 2025; Maydeu-Olivares \& Joe, 2008), where the first
three moments of \(X^2\) are matched to the first three moments of some
chi-square random variate, which is then used as the reference
distribution to conduct the test. The moments of \(X^2\) are estimated
using trace product formulae involving \(\diag(\bpi_2(\hat\theta))\) as
well as \(\hat\bOmega_2\). Though the Pearson test looks as if the
\(\bOmega_2\) matrix is not required, it is actually required to compute
the p-values.

More generally, any LIGOF test statistic can be constructed using
\(X^2 = \hat\be_2^\top \hat\bXi \hat\be_2\), where
\(\hat\bXi \xrightarrow{\text D} \bXi\) is some \(S\times S\) weight
matrix that can be arbitrarily chosen. We saw earlier that the Wald test
involves \(\hat\bXi=\hat\bOmega_2^{+}\), while the Pearson test involves
\(\hat\bXi = \diag(\bpi_2(\hat\theta))^{-1}\). Other choices for this
weight matrix are \(\hat\bXi = \bI\) (RSS test) or
\(\hat\bXi = \hat\bSigma_2^{-1}\) (Multinomial test).

\section{Usage}\label{usage}

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-agresti2002categorical}
Agresti, A. (2002). \emph{Categorical data analysis} (2nd ed).
Wiley-Interscience.

\bibitem[\citeproctext]{ref-alfonzetti2025pairwise}
Alfonzetti, G., Bellio, R., Chen, Y., \& Moustaki, I. (2025). Pairwise
stochastic approximation for confirmatory factor analysis of categorical
data. \emph{British Journal of Mathematical and Statistical Psychology},
\emph{78}(1), 22--43. \url{https://doi.org/10.1111/bmsp.12347}

\bibitem[\citeproctext]{ref-bartholomew2002goodness}
Bartholomew, D. J., \& Leung, S. O. (2002). A goodness of fit test for
sparse 2p contingency tables. \emph{British Journal of Mathematical and
Statistical Psychology}, \emph{55}(1), 1--15.

\bibitem[\citeproctext]{ref-bock1970fitting}
Bock, R. D., \& Lieberman, M. (1970). Fitting a {Response Model} for
{\emph{n}} {Dichotomously Scored Items}. \emph{Psychometrika},
\emph{35}(2), 179--197. \url{https://doi.org/10.1007/bf02291262}

\bibitem[\citeproctext]{ref-cai2006limitedinformation}
Cai, Li., Maydeu-Olivares, A., Coffman, D. L., \& Thissen, David.
(2006). Limited-information goodness-of-fit testing of item response
theory models for sparse 2 tables. \emph{British Journal of Mathematical
and Statistical Psychology}, \emph{59}(1), 173--194.
\url{https://doi.org/10.1348/000711005X66419}

\bibitem[\citeproctext]{ref-christoffersson1975factor}
Christoffersson, A. (1975). Factor analysis of dichotomized variables.
\emph{Psychometrika}, \emph{40}(1), 5--32.
\url{https://doi.org/10.1007/BF02291477}

\bibitem[\citeproctext]{ref-fuller2009introduction}
Fuller, W. A. (2009). \emph{Introduction to statistical time series}.
John Wiley \& Sons.

\bibitem[\citeproctext]{ref-huber1964robust}
Huber, P. J. (1964). Robust {Estimation} of a {Location Parameter}.
\emph{The Annals of Mathematical Statistics}, \emph{35}(1), 73--101.
\url{https://doi.org/10.1214/aoms/1177703732}

\bibitem[\citeproctext]{ref-jamil2025pairwise}
Jamil, H., Moustaki, I., \& Skinner, C. (2025). Pairwise likelihood
estimation and limited-information goodness-of-fit test statistics for
binary factor analysis models under complex survey sampling.
\emph{British Journal of Mathematical and Statistical Psychology},
\emph{78}(1), 258--285. \url{https://doi.org/10.1111/bmsp.12358}

\bibitem[\citeproctext]{ref-joreskog1990new}
Jöreskog, K. G. (1990). New developments in {LISREL}: Analysis of
ordinal variables using polychoric correlations and weighted least
squares. \emph{Quality and Quantity}, \emph{24}(4), 387--404.
\url{https://doi.org/10.1007/BF00152012}

\bibitem[\citeproctext]{ref-joreskog1994estimation}
Jöreskog, K. G. (1994). On the {Estimation} of {Polychoric Correlations}
and their {Asymptotic Covariance Matrix}. \emph{Psychometrika},
\emph{59}(3), 381--389. \url{https://doi.org/10.1007/BF02296131}

\bibitem[\citeproctext]{ref-joreskog2001factor}
Jöreskog, K. G., \& Moustaki, I. (2001). Factor {Analysis} of {Ordinal
Variables}: {A Comparison} of {Three Approaches}. \emph{Multivariate
Behavioral Research}, \emph{36}(3), 347--387.
\url{https://doi.org/10.1207/S15327906347-387}

\bibitem[\citeproctext]{ref-katsikatsou2012pairwise}
Katsikatsou, M., Moustaki, I., Yang-Wallentin, F., \& Jöreskog, K. G.
(2012). Pairwise likelihood estimation for factor analysis models with
ordinal data. \emph{Computational Statistics \& Data Analysis},
\emph{56}(12), 4243--4258.
\url{https://doi.org/10.1016/j.csda.2012.04.010}

\bibitem[\citeproctext]{ref-lord1968analysis}
Lord, F. M. (1968). An {Analysis} of the {Verbal Scholastic Aptitude
Test Using Birnbaum}'s {Three-Parameter Logistic Model}.
\emph{Educational and Psychological Measurement}, \emph{28}(4),
989--1020. \url{https://doi.org/10.1177/001316446802800401}

\bibitem[\citeproctext]{ref-maydeu2005limited}
Maydeu-Olivares, A., \& Joe, H. (2005). Limited- and full-information
estimation and goodness-of-fit testing in 2 n contingency tables: {A}
unified framework. \emph{Journal of the American Statistical
Association}, \emph{100}(471), 1009--1020.

\bibitem[\citeproctext]{ref-maydeu2006limited}
Maydeu-Olivares, A., \& Joe, H. (2006). Limited {Information
Goodness-of-fit Testing} in {Multidimensional Contingency Tables}.
\emph{Psychometrika}, \emph{71}(4), 713--732.
\url{https://doi.org/10.1007/s11336-005-1295-9}

\bibitem[\citeproctext]{ref-maydeu2008overview}
Maydeu-Olivares, A., \& Joe, H. (2008). An overview of limited
information goodness-of-fit testing in multidimensional contingency
tables. In K. Shigemasu, A. Okada, T. Imaizumi, \& T. Hoshino (Eds.),
\emph{New {Trends} in {Psychometrics}} (pp. 253--262). Universal Academy
Press.

\bibitem[\citeproctext]{ref-muthen1978contributions}
Muthén, B. (1978). Contributions to factor analysis of dichotomous
variables. \emph{Psychometrika}, \emph{43}(4), 551--560.
\url{https://doi.org/10.1007/BF02293813}

\bibitem[\citeproctext]{ref-muthen1984general}
Muthén, B. (1984). A general structural equation model with dichotomous,
ordered categorical, and continuous latent variable indicators.
\emph{Psychometrika}, \emph{49}(1), 115--132.
\url{https://doi.org/10.1007/BF02294210}

\bibitem[\citeproctext]{ref-newey1994large}
Newey, W. K., \& McFadden, D. (1994). Large sample estimation and
hypothesis testing. In R. F. Engle \& D. L. McFadden (Eds.),
\emph{Handbook of {Econometrics}} (Vol. 4, pp. 2111--2245). Elsevier.
\url{https://doi.org/10.1016/S1573-4412(05)80005-4}

\bibitem[\citeproctext]{ref-reiser1996analysis}
Reiser, M. (1996). Analysis of residuals for the multionmial item
response model. \emph{Psychometrika}, \emph{61}(3), 509--528.
\url{https://doi.org/10.1007/BF02294552}

\bibitem[\citeproctext]{ref-vandervaart1998asymptotic}
van der Vaart, A. W. (1998). \emph{Asymptotic {Statistics}}. Cambridge
University Press. \url{https://doi.org/10.1017/CBO9780511802256}

\end{CSLReferences}

\section*{Acknowledgements}\label{acknowledgements}
\addcontentsline{toc}{section}{Acknowledgements}

I thank Rabi'ah Roslan for her diligent contributions as part of her
undergraduate project and for the insightful discussions that helped
shape this paper.




\end{document}
