---
bibliography: refs.bib
csl: apa.csl
---

{{< include _extensions/_maths_shortcuts.qmd >}}

```{r}
#| include: false
library(tidyverse)
library(kableExtra)
library(gt)
```

### Ordinal data

Consider the case of analysing multivariate data $\mathbf y = (y_{1}, \ldots, y_{p})^\top$, where each item $y_{i}$ is an ordinal random variable with $m_i$ categories, $i=1,\dots,p$.
Let $\mathcal R = \{ \mathbf c = (c_1,\dots, c_p)^\top \mid c_i \in \{1,\dots, m_i\}\}$ be the set of all possible response patterns, and let $R=\prod_{i} m_i$ be the cardinality of this set.
The joint probability of observing a response pattern $\mathbf c_r \in \mathcal R$ is given by
$$
\pi_r = \Pr(\mathbf y = \mathbf c_r) = \Pr(y_1 = \mathbf c_{r1}, \ldots, y_p = \mathbf c_{rp}), \hspace{2em} r = 1, \ldots, R,
$${#eq-each-joint-resp-prob}
with $\sum_r \pi_R = 1$.
Collect all response probabilities into the vector $\boldsymbol \pi = (\pi_1, \ldots, \pi_R)^\top \in [0,1]^R$.
An example with $p=3$, $m_1=2$, and $m_2=m_3=3$ is given below.
In total, there are $R=2 \times 3 \times 3 = 18$ response patterns as shown in @tbl-response-patterns.

```{r}
#| include: false

tab_rp <-
  expand_grid(
    y1 = 1:2,
    y2 = 1:3,
    y3 = 1:3
  ) |> 
  unite("pattern", everything(), sep = "", remove = FALSE) |>
  mutate(r = row_number()) |>
  select(r, starts_with("y"), pattern)
```

::: {#tbl-response-patterns layout-ncol=2}

```{r}
#| html-table-processing: none
#| echo: false
tab_rp |>
  slice(1:9) |>
  gt() |>
  cols_label(
    r = md("$r$"),
    y1 = md("$y_1$"),
    y2 = md("$y_2$"),
    y3 = md("$y_3$"),
    pattern = "Pattern"
  ) |>
  tab_options(table.width = "80%")
```

```{r}
#| html-table-processing: none
#| echo: false
tab_rp |>
  slice(-(1:9)) |>
  gt() |>
  cols_label(
    r = md("$r$"),
    y1 = md("$y_1$"),
    y2 = md("$y_2$"),
    y3 = md("$y_3$"),
    pattern = "Pattern"
  ) |>
  tab_options(table.width = "80%")
```

Response patterns for $p=3$ with $m_1=2$, and $m_2=m_3=3$.
:::

Later on we wish to use lower-order residuals to assess the fit of a model to the data, which first requires a description of lower-order moments and its connection to the joint response probabilities.
<!-- Let $I_{ik} = [y_i = k]$ be the indicator variable for the event that $y_i$ takes the value $k$, where $[\cdot]$ is the Iverson bracket. -->
Marginally, each $y_i$ can be viewed as a multinoulli random variable with event probabilities $\pi^{(i)}_k = \Pr(y_i = k)$, $k=1,\dots m_i$, that sum to one.
Therefore, this univariate distribution is characterised by its $(m_i-1)$ *moments* $\pi^{(i)}_2,\dots,\pi^{(i)}_{m_i}$, with the first moment being redundant due to the sum to unity constraint.
All univariate moments can be collected into the vector $\dot{\boldsymbol\pi}_1 = (\pi^{(i)}_k)^\top$ whose dimension is $S_1 = \sum_i (m_i-1)$.
In a similar light, the bivariate distribution of $(y_i, y_j)$ is characterised by its $(m_i-1)(m_j-1)$ *joint moments* $\pi^{(ij)}_{k,l} = \Pr(y_i = k, y_j = l)$, $k=2,\dots,m_i$, $l=2,\dots,m_j$.
Also collect all bivariate moments into the vector $\dot{\boldsymbol\pi}_2 = (\pi^{(ij)}_{k,l})^\top$ whose dimension is $S_2 = \sum_{i<j} (m_i-1)(m_j-1)$.
Finally, denote by $\boldsymbol\pi_2 = (\dot\bpi_1^\top, \dot\bpi_2^\top)^\top$ the vector of multivariate moments up to order 2, which is a vector of length $S = S_1 + S_2$.

Because the lower order moments are contained in the higher order moments, the vector $\boldsymbol\pi_2$ can be extracted from the joint probabilities $\bpi$ via a linear operation $\bpi_2 = \bT_2 \bpi$ [@jamil2025pairwise].
As an example, continuing from the $p=3$ instance above, the moments for the first variable $y_1$, $\Pr(y_1=2)$ can be obtained by *summing* over all joint probabilities whose patterns contain $y_1=2$.
The positions of these joint probabilities in the vector $\bpi$ are picked up by the first row of the matrix $\bT_2$.
Similarly, the two bivariate moments of $(y_1,y_2)$, i.e. $\pi^{(12)}_{22}$ and $\pi^{(12)}_{23}$ are obtained by summing over the joint probabilities whose patterns contain $y_1=2$ and $y_2=2$, and $y_1=2$ and $y_2=3$, respectively.

::: {#fig-T2-matrix}
```{r}
#| echo: false
options(width = 100)
create_T2_mat <- function(m) {
  # m: integer vector of length p, where m[i] = number of categories of variable i
  p <- length(m)
  # 1) all joint patterns (rows = ∏ m[i], cols = p)
  patterns <- expand.grid(rev(lapply(m, seq_len)), KEEP.OUT.ATTRS = FALSE, stringsAsFactors = FALSE)
  patterns <- patterns[, rev(seq_len(p))] # reverse to match y1, y2, ...
  n_pat <- nrow(patterns)
  
  # 2) precompute total number of rows: sum_i (m[i]-1) + sum_{i<j} (m[i]-1)*(m[j]-1)
  uni_rows <- sum(m - 1)
  biv_rows <- 0L
  for(i in seq_len(p-1)) for(j in (i+1):p)
    biv_rows <- biv_rows + (m[i]-1)*(m[j]-1)
  total_rows <- uni_rows + biv_rows
  
  # 3) allocate
  out <- matrix(0L, nrow = total_rows, ncol = n_pat)
  rn  <- character(total_rows)
  
  # 4) fill univariate indicator rows
  r <- 1L
  for(i in seq_len(p)) {
    for(cat in 2:m[i]) {
      out[r, ] <- as.integer(patterns[[i]] == cat)
      rn[r]   <- paste0("Y", i, "=", cat)
      r       <- r + 1L
    }
  }
  
  # 5) fill bivariate indicator rows
  for(i in seq_len(p-1)) for(j in (i+1):p) {
    for(c1 in 2:m[i]) for(c2 in 2:m[j]) {
      out[r, ] <- as.integer(patterns[[i]] == c1 & patterns[[j]] == c2)
      rn[r]   <- paste0("Y", i, "=", c1, ",Y", j, "=", c2)
      r       <- r + 1L
    }
  }
  
  rownames(out) <- rn
  colnames(out) <- apply(patterns, 1, paste0, collapse = "")
  out
}
create_T2_mat(c(2, 3, 3))
```

Matrix $\bT_2$ for the case of $p=3$ with $m_1=2$, and $m_2=m_3=3$.

:::

Note that this construction of lower-order moments generalises to any order $q \le p$, but the total number of moments up to order $q$ grows combinatorially in both $p$ and the category counts $m_i$, yielding design matrices $\mathbf{T}_q$ that can become computationally burdensome. 
Moreover, although we arbitrarily dropped the first moment in the foregoing construction, the choice of which category to omit is immaterial.
This is because category probabilities sum to one, so excluding any one category produces a similar-dimensional parameterisation algebraically equivalent to excluding any other. 
For further details, consult @reiser1996analysis and @maydeu2006limited.

### Confirmatory factor analysis

The confirmatory factor analysis (CFA) model imposes a structure on the joint response probabilities by assuming that the $p$ observed variables are manifestations of a smaller set of $q$ latent variables.
In this way, the CFA may be viewed as a data-reduction technique since, effectively, the correlations among variables are modelled by a pre-specific factor structure using lower-dimensional data summaries.

CFA is typically used for continuous manifest variables, but it can also be applied to ordinal data.
A common approach is the *underlying variable* (UV) approach, where the observed responses $y_i$ are assumed to be discretised versions of continuous latent variables $y_i^*$.
The connection is made through
$$
y_i = \begin{cases}
1 & \ \ \tau_0^{(i)} < y^*_i < \tau_1^{(i)} \\
2 &  \ \ \tau_{1}^{(i)} <  y^*_i < \tau_2^{(i)} \\
3 &  \ \ \tau_{2}^{(i)} <  y^*_i < \tau_3^{(i)} \\
\vdots &  \hphantom{\tau_{1}^{(i)} \leq \ \ \ } \vdots \\
m_i & \tau_{m_i-1}^{(i)} < y^*_i < \tau_{m_i}^{(i)},
\end{cases}
$$
with the *thresholds* $\tau_k^{(i)}$ for item $i$ satisfying the ordering 
$$
-\infty \equiv \tau_0^{(i)} < \tau_1^{(i)} < \tau_2^{(i)} < \cdots < \tau_{m_i-1}^{(i)} < \tau_m^{(i)} \equiv +\infty.
$$
Evidently, the model is invariant to a linear transformation, since scaling and shifting the underlying variables $y_i^*$ do not affect the outcome of the ordinal variable $y_i$.
For this reason it is convenient to assume, for the purposes of model identifiability, a zero mean Gaussian distribution $\by^* \sim \N_p(\bzero,\bSigma_{\by^*})$, where $\bSigma_{\by^*}$ is a correlation matrix.

The underlying continuous variables $\by^*$, unlike their discrete counterparts $\by$, are now suitable to be modelled using a factor analysis model.
Here, the goal is to find a set of latent factors $\bfeta = (\eta_1,\dots,\eta_q)^\top \in \bbR^q$, with $q \ll p$, that sufficiently explain the covariance structure of the $p$-dimensional variable space.
This is achieved by the relationship
$$
\by^* = \bLambda \bfeta + \bepsilon,
$$
where $\bLambda$ is a (often sparse) $p \times q$ matrix of factor loadings, and $\bepsilon$ is a vector of residuals.
Certain distributional assumptions are made, namely that $\bfeta \sim \N_q(\bzero,\bPsi)$ with $\bPsi$ a correlation matrix, $\bepsilon \sim \N_p(\bzero,\bTheta_{\bepsilon})$ with $\bTheta_{\bepsilon} = \bI - \diag(\bLambda \bPsi \bLambda^\top)$, and that $\Cov(\bfeta,\bepsilon) = \bzero$.
Together, this implies that the polychoric correlation matrix of $\by$ is given by
$$
\bSigma_{\by^*} = \bLambda \bPsi \bLambda^\top + \bTheta_{\bepsilon} \in \bbR^{p\times p}.
$$
As a remark, the UV approach is commonly employed in the context of confirmatory factor analysis (CFA) models due to the ease of modelling, though other approaches such as item response theory (IRT) models are also available [@joreskog2001factor].

For this factor analysis model, the parameters of interest are the non-zero entries $\blambda$ of the loading matrix $\bLambda$, the unique non-diagonal entries $\bpsi$ in the factor correlation matrix $\bPsi$, and the thresholds $\btau^{(i)} = (\tau_1^{(i)},\dots,\tau_{m_i-1}^{(i)})^\top$ for each ordinal item $y_i$.
Collectively, these parameters are denoted by $\theta = (\blambda^\top,\brho^\top,\btau^{(1)},\dots,\btau^{(p)})^\top$ belonging to some parameter space $\Theta$.
Under this CFA model, each joint response probability $\pi_r$ from @eq-each-joint-resp-prob is now evaluated as a function of  $\theta$:
$$
\pi_r := \pi_r(\theta) = \idotsint \limits_{\mathcal C_r} \phi_p(\by^* \mid \bzero,\bSigma_{\by^*}) \, \dint\by^*,
$$
where the $p$-dimensional integral is taken over the set $\mathcal C_r = \{ \by^* \in \bbR^p \mid y_i = \mathbf c_{ri}, i=1,\dots,p\}$, the set of all continuous values that yield the response pattern $\mathbf c_r$.

Many methods exist...

- This includes minimum variance or best asymptotic  normal (BAN) estimators such as the maximum likelihood estimator (MLE) or the  minimum chi-square estimator. It also includes the limited information estimators  for IRT models: those implemented in programs such as LISREL, EQS, MPLUS,  or NOHARM, and those proposed by Christoffersson (1975) and J ̈oreskog and  Moustaki (2001).
- Also, the limited information estimators considered by Christoffersson (1975), J ̈oreskog (1994); see also Maydeu-Olivares (2006), J ̈oreskog and Moustaki (2001), Lee, Poon, and Bentler (1995), Maydeu-Olivares (2001b), and Muth ́en (1978, 1984, 1993) are special cases of this framework.
- (e.g., Christoffersson 1975; Muthén 1978, 1984, 1993).

They have in common that the estimating equations take the same basic form 
$$
U(\theta) \propto \Delta_\theta^\top W_\theta m_n(\theta)
$$
where $\Delta_\theta$ is the Jacobian matrix containing the derivatives of the model moments with respect to the parameters, $W_\theta$ is a weight matrix, and $m_n(\theta)$ is the vector of differences between the observed and model-implied moments.

### Parameter estimation

Suppose that a sample $\mathcal Y = \{\by^{(s)}\}_{s=1}^n$ is obtained, where $\by^{(s)} = (y_1^{(s)},\ldots,y_p^{(s)})^\top$ represents the $p$-dimensional ordinal-data observation from subject $s\in\{1,\dots,n\}$.
As a remark, samples may not necessarily be independent, and in such cases, corresponding sampling weights $\omega_s$ can be used to account for the sampling design [@jamil2025pairwise], and most of what will be discussed below can be adapted to account for this.



### Distribution of residuals

$$
\sqrt{n} (\bp - \bpi) \xrightarrow{D} {\N}_R(\bzero, \bSigma)
$${#eq-clt-prop}

### Wald-type tests

### Pearson-type tests

### General GOF tests

### Estimation of degrees of freedom

## References